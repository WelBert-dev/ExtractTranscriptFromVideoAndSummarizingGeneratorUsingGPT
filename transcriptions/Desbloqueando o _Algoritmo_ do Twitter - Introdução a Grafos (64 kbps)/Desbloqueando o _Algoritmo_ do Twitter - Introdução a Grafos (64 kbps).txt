 Olá pessoal, Fabio Akita. Já não é notícia nova e a essa altura acho que todo mundo já viu ou ouviu alguém comentando sobre o código dos algoritmos de recomendação do Twitter que o tio Elon Musk liberou parcialmente no último dia de março de 2023. No momento em que eu estava escrevendo o script desse episódio não se passou uma semana ainda e até a gravação desse vídeo ainda não foi liberada a segunda parte do código. Mas vamos ver o que temos até agora. No mesmo dia que a primeira parte do código foi liberado eu e diversos outros programadores e pesquisadores começamos a avaliar o código e surgiu muita coisa interessante. Eu mesmo fui tweetando em tempo real o que eu achei de primeiras impressões. O meu foco foi mais na parte técnica e arquitetura. Os links pros meus tweets originais estão na descrição abaixo e nesse vídeo eu vou elaborar em mais detalhes o que não coube em pequenos tweets. Antes de começar eu preciso deixar alguns avisos muito claros aqui. Esse código é incompleto ainda falta pelo menos uma segunda parte e mesmo com essa parte ainda não vamos ter certeza que tudo vai ser liberado. O que o Twitter soltou não é mais uma pequena fração de todo o código necessário para se fazer um verdadeiro clone de Twitter. Esse código só cobre uma parte dos sistemas que devolvem os tweets recomendados que você vê na aba de pra você. Em termos coloquiais chamamos isso tudo de algoritmo do Twitter mas não é um único algoritmo a coleção de dezenas de algoritmos e heurísticas que funcionando juntos geram recomendações que têm relevância para os seus interesses. Mesmo sendo incompleto estamos falando de mais de quatro mil e seiscentos arquivos mais de trezentas e trinta mil linhas de código sendo mais na metade código feito na linguagem escala e quase trinta por cento em Java. A próxima linguagem mais relevante depois dessas duas é Rust na nova parte de machine learning chamada nave mas mesmo assim é só dois por cento do total desse código. Mais importante o sistema de build que eles utilizam é o Beisel. Pense npm se você for de JavaScript ou algo parecido com Gradle se for de Java mesmo. Eu mesmo nunca vi Beisel sendo utilizado num projeto de verdade e cada subprojeto tem um arquivo de build em Beisel mas eles omitiram os arquivos workspace que são necessários para conseguirmos realmente compilar e buildar cada projeto. É possível reconstruir do zero via tentativa e erro mas é um trabalhinho bem chato que eu não tenho interesse em fazer e até o momento eu não vi ninguém se prontificando a fazer. Portanto tudo que eu ou qualquer outro analisar é baseado unicamente em análise estática do código que é um jeito mais bonito de dizer que só temos como ler o código ou fazer compilações parciais. Por isso não acredite em tudo que ler em somente um pedaço do código. Deixa eu dar um exemplo. Alguns pesquisadores foram mais interessados na parte das regras de negócio. Os critérios que afetam o alcance de tweets. Em resumo não teve nenhuma grande novidade bombástica além do que todo mundo mais ou menos já sabia por observação. Vamos ver dois exemplos. O blog post do desenvolvedor Steven da Vercel e o blog post do Akash Gupta. Ambos os links novamente estão na descrição abaixo. No post do Steven você vai ler que o algoritmo de ranqueamento vai determinar pesos em algumas ações em cima do seu tweet. Por exemplo um like tem peso 0.5. O retweet é mais importante tem peso 1. Se alguém clicar no tweet der reply ou like ou fica mais que dois minutos lendo vai ter peso 12 e assim por diante. Mas de onde que ele tirou essa informação. Ele coloca o link para o Rhythm no diretório Projects Home Recap dentro do repositório de Algorithm ML que é o repositório menor de Machine Learning. A contagem de Scala e Java que falei acima foi no repositório principal de Algorithm que é o maior mas tem esse outro menor que é basicamente um projetinho Python com 91 arquivos e umas 6 mil linhas de código para PyTorch que eles chamam de Heavy Ranker. O problema é que o único lugar nesse código onde esses números de peso aparecem é na documentação desse Rhythm. Olha só se eu abrir o VS Code no projeto de Algorithm ML e procurar por esse parâmetro is negative feedback v2 que tem um peso negativo 74 só aparece no Rhythm em nenhum lugar no código tem de fato esse valor. Existe um arquivo localprod.yaml que parece que configura esses parâmetros para a produção e no servidor deles pode ser que esteja mesmo esse número mas no arquivo nesse repositório não tem nada portanto eu não posso dizer que tá nem certo nem errado só que é inconclusivo. Só que se você ler o post vai achar que isso é um fato e eu tenho certeza que um monte de gente vai referenciar esse blog post e repetir a informação sem checar o que significa o Steven não é rua nem mentiu mas ele tá descrevendo informação incompleta. Mas não é só isso se for agora no post do tal gupta ele vai falar de outros parâmetros de peso por exemplo ele fala que retweets dão boost de 20 vezes e de onde que ele tira isso é do repositório principal de Algorithm no arquivo Early Bird TensorFlow Basis Similariary Engine.Scala no projeto CR Mixer. Lá tem esse trecho da função get linear ranking params e agora não é um ritme é trecho de código então deve ser confiável certo também não. Vamos procurar quem chama essa função caímos nesse trecho da função get relevance options que caso essa variável use TensorFlow ranking seja falsa daí vai chamar a função com os parâmetros que vimos antes caso contrário vai chamar essa outra função get tensorflow Basis ranking params e agora vamos ver o que ela define voltamos pro arquivo anterior e vemos que ela chama a função get linear ranking params abaixo mas muda o tipo e mais importante tem esse apply boosts pra false. O que isso significa será que então todos esses parâmetros com o nome boost aqui embaixo vão ser ignorados como offensive boost ou tweet restrange boost aí que tá não temos como saber como eu disse antes esse código é incompleto e até o presente momento não sabemos nem se 100% desse código compila não sabemos quais dependências estão faltando não sabemos como buildar e portanto não sabemos como executar esse código e nem quais caminhos dentro são realmente executados pra cada tipo de dados e configurações e mesmo se conseguirmos fazer rodar vamos voltar pra aquele ritme que eu mostrei agora a pouco da parte de machine learning com pytorch olha o que diz não podemos liberar os dados reais de treino por causa de privacidade faz sentido claro mas sem ter o mesmo material de treino ou pelo menos o mesmo modelo final não temos como reproduzir exatamente o que roda no twitter e portanto não podemos garantir que é exatamente a mesma coisa rodando em produção nos servidores deles portanto do ponto de vista de auditoria podemos confiar que sim esse código é certamente retirado do código real dentro do twitter é código demais pra falsificar não seria prático mas os detalhes do que exatamente roda lá dentro talvez a gente nunca saiba sempre é possível omitir camadas de filtros na fase de mixagem que adicionam ou removem tweets ou omitir os arquivos de configuração reais que modificam todos esses pesos e boosts não temos como realmente saber por isso cuidado com afirmações sendo feitas só além do esse código e antes de continuar eu preciso falar da vergonha alheia que foi o comportamento geral das pessoas frente a esse código vamos estabelecer eu não sou fã de ilomasque doge coin é cheat coin e eu acho uma puta perda de tempo gente que idolatra o cara ou que perde tempo engajando ele é só mais um cara que existe a existência dele não me afeta assim como a existência de qualquer outro pra mim é insignificante e é só isso o twitter é uma empresa privada como qualquer empresa ela não tem nenhuma obrigação de liberar código portanto o que ela fez ao liberar pra mim é um excelente marketing uma cortina de fumaça pra desviar a atenção controlar a narrativa e deixar todo mundo coçando a cabeça tentando desvendar esse quebra cabeça e ao mesmo tempo cutucar os críticos no governo e cutucar os concorrentes como um zucker berg da vida no mínimo vai deixar muita gente chata ocupada tentando abrir uma caixa semitransparente que não tem chave pra tranca a real motivação por trás disso não me interessa só me interessa que temos esse código o que foi liberado continua sendo um código excelente de explorar porém pessoas idiotas parecem que tem talento de encontrar parte mais insignificante e viralizar os memes mais idiotas se você pesquisar a palavra ilon move s code a primeira coisa que vai aparecer é essa chave alter e ilon logo em cima de outras chaves suspeitas como alter e democrat ou alter e republicão o que vazamento inesperado o que é isso a prova que o ilon da boost nos tweets dele o que é isso de democratas republicanos tem manipulação neles também até gente que se auto denomina programador ficou fazendo meme disso é uma vergonha alheia se você viu alguém fazendo meme disso pode desseguir o cara é inútil deixa eu mostrar como faz vamos lá pro código esse alter e ilon é chave pra um mapa que no final vai se chamar predicate map que é gerado através desta sequência onde cada elemento é um tempo com uma string que serve de chave e um feature map que é uma função boleana no caso desse trecho do ilon ele recebe um candidato que é provavelmente um candidato de um comendar e checa se a tal feature de alter contém esse tal ddg estets ilon feature ênfase em estets bacana e quem usa esse predicate map vamos procurar no código achamos aqui na linha 82 do objeto serve de events builder que estende essa traite client events builder nesse trecho ele está iterando por cada predicado naquela predicate map e no final o que está gerando são os pontos para cada predicado é um contador e quem que vai usar essas contagens vamos de novo pesquisar agora por esse objeto serve de events builder pronto chegamos nesse home scribe client event side effect é o que twitter chama de side effect pelo que eu já tinha deduzido lendo o código side effect é um job de cáfica um job acíncron e scribe é justamente o servidor de logs open source que foi desenvolvido pelo facebook é só um servidor de log não diferente de uma log flume ou log estech ou rc log da vida esses códigos servem para gerar métricas é só isso ah mas tinha que ser o cusão egocêntrico do ilon para ter um código especial para uma métrica só para ele o é mesmo se for se ele desemboçou 40 bilhões para comprar porra toda achei que foi muito pouco código com o nome dele achei que fosse achar classes inteiras renomeadas para doge coin ou algo assim espalhado por aí aliás vocês estão assumindo que esse código foi feito por o time do ilon outra teoria muito mais plausível é que esse código foi feito justamente pelo povo que ele demitiu para monitorar os efeitos dos tweets do ilon antes da compra ter sido realizada muito mais plausível independente do que for só vamos saber se alguém de dentro do twitter vazar os logs do repositório git de verdade com o time stamp do commit e mesmo assim com o mito de git podem ser facilmente adulterados por essa discussão meio inútil todo mundo ficou fazendo um código de log e vocês se acham programadores a discussão do porque esse trecho existe é irrelevante e só serve para demonstrar como é um programador que gosta de fazer bike shed discutir coisas inúteis sem nenhum valor sem nenhum mérito você está perdendo seu tempo agora vamos entender como um programador de verdade olha para esse código o objetivo a partir daqui é o seguinte desmistificar a diferença de código de curso código de tutorial e código de produção de empresa de verdade para um iniciante que nunca trabalhou ou só trabalhou em projetos pequenos a diferença vai ser gritante a outra coisa é como um programador senior pensa infelizmente não tem como num único vídeo com só dois dias de análise sem compilar consegui ter a visão completa então tudo que eu vou dizer são só minhas primeiras impressões eu já disse isso em outros vídeos mas aqui vale repetir código de curso código de tutorial código em vídeo no youtube são absurdamente diferentes de código de verdade de produção não tem nem comparação a complexidade e tamanho são impossíveis de caber em qualquer curso ou tutorial é inviável literalmente sem exagero um curso vai te ensinar a fazer uma casinha de rato com caixa de sapato mas todo projeto relevante de verdade é um burro califa o arranha ser o mais alto do mundo que nunca foi construído e toda a engenharia para construir isso num terreno de areia sem receita pré pronta outra coisa que eu já disse antes software nunca acaba se seu produto realmente vale alguma coisa você vai reescrever partos ou até ele inteiro mais cedo ou mais tarde e com esse código liberado pelo twitter eu vou finalmente poder ilustrar esses dois pontos e esse vai ser o objetivo do vídeo eu peguei aleatoriamente o primeiro projeto no github com o nome de twitter clone tem dezenas por aí esse é feito em next de s com type script firebase e teio ind só tecnologia de ponta como o amador poderia pensar e ele faz tudo olha só o ritmo os usuários conseguem postar tweets da like retweet reply adicionar bookmarks adicionar imagens segui o de seguir outros usuários updates de like e tudo mais em tempo real porra faz tudo só que não isso não passa de uma caixa de sapatos frente ao burro de fita vamos ver com a ferramenta toquei vemos que esse projeto tem 158 arquivos e mais de 32 mil linhas de código para iniciante sem dúvida pode parecer gigantesco mas não é mesmo sendo só uma fração incompleta o repositório do twitter que estamos avaliando tem mais de 4.600 arquivos e mais de 330 mil linhas de código 10 vezes mais código mas esses projetos se quer são equivalentes O Cone é só o front-end e só as operações mais básicas, ele jamais aguentaria a escala do Twitter em produção. O Dialgorithm, que foi liberado, é só para buscar os tweets que aparecem na aba para você. Notaram como esses Cones omitem essa aba para você? Claro, porque justamente há inteligência para buscar tweets recomendados. Em tempo real, em escala não é um problema trivial de resolver. Todo Cone que você achar ou vai omitir ou vai ter uma versão super besta de recomendação e nunca vão escalar para ordem de centenas de milhões de pessoas. São todos caixas de sapato. E não tem nenhum problema nisso. O foco da maioria desses Cones é ser um exercício para iniciantes de front-end treinarem e fazer interfaces parecidas com coisas que já existem. Nada mais, nada menos. Nenhum desses Cones, seja Cone de Twitter, Cone de Instagram, Cone de Pinterest, chega perto da unha dos pés das versões de verdade e são incapazes de fazer uma fração. Do que os originais fazem ou de aguentar essa fração do tráfico. Essa é a realidade dos Cones. São todos meros exercícios. Com tanto que vocês entendam isso, não tem nenhum problema se exercitar neles. Todo mundo precisa começar do zero. E esse é o nível zero. A razão dessa diferença é que é trivial fazer qualquer aplicativo funcionar só na sua máquina com uma pessoa usando. Mesmo se você virar freelancer, normalmente trabalha em projetos que muito pouca gente usa. Dez pessoas, cem pessoas, mesmo mil pessoas é extremamente pequeno. Empresas grandes como Mercado Livre, iFood, Nubank, Lino, com milhões, dezenas de milhões de pessoas. No caso de Facebook, Google ou Apple, são bilhões de pessoas. E nessa escala, as soluções são completamente diferentes de todo o curso e tutorial que você procurar. Eu afirmo que não existe nenhum curso que cobre casos de larga escala. Simplesmente porque todos fazendo cursos nunca tiveram que lidar com esse problema. Eles também simplesmente não sabem que isso existe. Eu mesmo nunca tive que lidar com o caso excepcional de bilhões de pessoas. É muito caso a caso. Em cada empresa gigante vai ser diferente. Nenhum curso conseguiria cobrir, mesmo se quisessem. Simplesmente não existe esse material disponível. E é uma das razões de porque esse código do Twitter é relevante. Porque não existe código de produção de empresa de verdade disponível para podermos mostrar todos são fechados. Eu trabalhei em vários projetos, incluindo alguns dessa categoria grande. Mas eu não posso mostrar porque nenhum de nós deve quebrar confidencialidade e propriedade intelectual de clientes. Por isso eu falo pra não confiar em nenhum programador que não tenha tido esse nível de experiência. Eles nunca viram nada além da dimensão de caixa de sapato e acham que o resto do mundo é pequeno assim e definitivamente não é. Só quem já subiu no topo do Burj Khalifa sabe a diferença. A tal aba pra você, a primeira vista parece super simples. Na aba seguindo mostra só tweets de pessoas que eu sigo provavelmente em ordem cronológica. Fácil de fazer, um iniciante consegue imaginar um select top 100, asterisco, front tweets, order by, create a jet, desk da vida. Agora, no pra você tem tweets de gente que eu sigo e de gente que eu não sigo. Mas que as pessoas que eu sigo interagiram. Se eu sigo o João, e o João retuitou o tweet do Paulo, no meu pra você pode aparecer o tweet do Paulo. Só que não é tão simples assim. E se quem eu sigo interagiu com 100 tweets, quais deles deveriam aparecer pra mim? Das pessoas que eu sigo, quais delas são mais relevantes pra aparecer no topo primeiro? Porque eu vejo o like de algumas pessoas mas não de outras. Ah, será que como os postes de blog estavam tentando explicar é só botar um peso pra like, outro peso pra retweet e dar um group buy ou order buy da vida e pegar o topo? Também não. Como é impossível cobrir o código todo que foi liberado, eu escolhi começar falando só de um pequeno trecho. Um banco de dados no SQL de grafos que inclusive já tinha sido liberado antes como open source com licença à paixa chamado Graph.Jet e a história por trás dele. Tudo isso foi descrito num paper de autoria de Anish Sharma e vários outros colaboradores publicado em 2016. Pra todo mundo que não fez ciência da computação entender, na faculdade a gente aprende sobre um troço chamado Graphos. Na prática você já deve ter visto e até desenhado. É quando você desenha caixinhas ou círculos que chamamos de nós e ligamos um nó com outro nó com um troço que chamamos de vértices ou bordas. Podemos representar Graphos como matrizes adjacentes ou listas adjacentes. Outra forma de representar Graphos é com vetores. Daí podemos classificar Graphos baseados em características como número de nós ou número de vértices, edis ou bordas, direção das bordas e tem diversos tipos de Graphos como unidirecionais. Em Git por exemplo você já deve ter ouvido falar que é um DAG ou um Direct Acíclic Graph ou Graph Acíclico que não é cíclico e direto. Dado um Grapho temos como navegar de um nó pra outro nó. Pense em nós como uma casa numa rua e os vértices como o caminho talvez a rua entre essas casas. Algoritmos para navegar como o algoritmo do menor caminho entre dois nós que é o Hello World de Graphos. E não é só isso, ainda no assunto de classificação temos nós que ficam nas bordas do Grapho que tem poucos vértices e temos nós no meio dos Graphos com muitos vértices. Costumamos chamar esses nós de hubs. Na prática numa rede social chamamos de hubs mas pessoas comuns chamam de influencers e pessoas comuns são os nós das bordas com poucos relacionamentos ou vértices. Tudo na natureza pode ser representado com Graphos. Como eu falei antes evolução de desenvolvimento do software pode ser representado como um DAG, um Grapho de comits com branches. Um sistema de logística entregas é orientado num mapa com Graphos pra determinar o melhor caminho de entrega entre dois pontos. O DNA humano pode ser um Grapho onde os nós são as bases como a denina, citosina, goninitimina formando núcleo bases. As sinapses do nosso cérebro são Graphos. Num ecossistema animal temos a hierarquia da cadeia alimentar que pode ser representado como um Grapho. No mercado temos Graphos de empresas e relacionamentos de comprador e vendedor e assim por diante. No caso do Twitter que é uma rede social os nós costumam ser as pessoas e os vértices são os relacionamentos. Quem nós seguimos, quem damos likes ou retweets, quem bloqueamos e assim por diante. Se você nunca estudou ciência da computação vai ter um pouco de dificuldade de entender o que eu vou explicar e certamente não vai entender nada do código do Twitter. Eu vou tentar simplificar o máximo que der mas eu só consigo chegar até certo ponto. Então se interessa em depois e procurar cursos como nas aulas do MIT sobre Graphos. Pra operar em Graphos você vai precisar aprender coisas como álgebra linear e cálculo, pelo menos o cálculo básico. E portanto sim pra entender como uma rede social funciona e o código por trás precisa entender esses aspectos da matemática. E pra chegar no ponto de recomendação também precisa entender o básico de estatística e probabilidade porque recomendação é justamente a probabilidade dos tweets que o sistema recomendar realmente te interessar a ponto de engajar da like ou reply que é a base de machine learning. Mesmo se você não foi implementado o zero, inteligência artificial, machine learning, ainda assim precisa entender essa matemática. Machine learning é nada mais nada menos do que uma coleção de algoritmos de classificação e probabilidade. Se não souber quando pode usar regressão linear, regressão logística, similaridade de cosseno, base engino, não vai saber o que fazer. Cada caso, cada tipo de conjunto de dados e o resultado que quer depende de saber escolher quais desses algoritmos usar e pra saber quais usar precisa saber a matemática por trás. Você pode usar regressão linear quando não podia e ter resultados errados, que é a coisa mais comum de ver a amador fazer. Nem tudo que parece uma reta é uma reta. E no caso desse código do Twitter, sim, eu encontrei que ele usa coisas como grafos, regressão logística e similaridade de cosseno. Mas eu to me adiantando. Vamos voltar ao paper do GraphJet de 2016. Um problema que não é trivial de resolver e não tem uma solução única porque depende do comportamento dos usuários e dos dados disponíveis é recomendação, seja recomendação de produtos no e-commerce, recomendações de leitura nos sites de notícias ou recomendação de tweets num Twitter. Como usuários seguem usuários, naturalmente temos grafos, em particular grafos bipartidos, com usuários de um lado e tweets de outro. Os esforços de recomendação começaram no Twitter por volta de 2010, lembrando que o Twitter começou a operar em março de 2006. Eu mesmo queria em minha conta só um ano depois, em abril de 2007, só fui usar durante a RailsConf 2008 em Portland, onde eu via todo mundo se cadastrando pra gente compartilhar onde ia ser o próximo Happy Hour. Lembrando que ainda não existia Messenger do Facebook, não existia WhatsApp, não existia Telegram, o único aplicativo de mensagem que todo mundo usava era SMS. De qualquer forma, quando eles começaram a experimentar, a recomendar usuários novos pra gente seguir por volta de 2010, foi com um sistema que chamaram carinhosamente de WTF. What the fuck? Mentira. Era rootfollow. A arquitetura era como nessa figura. Segundo o paper, no seu núcleo tinha esse Castlevary, que é um motor de processamento de grafos em memory, ou seja, que mantém os grafos em memória. Isso é importante porque a maioria dos algoritmos conhecidos pra lidar com grafos, assume ter o grafo todo em memória. Guarde essa informação. Esse projeto, Castlevary, é open source e podemos encontrar no GitHub deles. Última atualização foi de 2021 e não é mais utilizado hoje. Continuando, esse Castlevary operava em snapshots de grafos de seguidores carregados do HDFS, que é o sistema de arquivos distribuídos do Hadoop. Quem já lidou com Big Data, Data Lakes, certamente já esbarrou em instalações de HDFS e no Twitter não era diferente. Só entenda que é um sistema de arquivos distribuído, como se você tivesse uma parte de redação única e espalhada em múltiplos servidores. É um nível acima de um simples RAID. Os grafos de seguidores vinham de outro banco de dados, chamado FlockDB, que fazia a ingestão desses dados no HDFS. FlockDB, acho que foi o primeiro banco de dados de grafos que o Twitter criou e soltou como open source. O que aconteceu foi, eles começaram com qualquer tech startup normal sem saber o que o produto ia se tornar. No começo, usavam um banco de dados normal, mais psico ao mesmo. Mas quando começaram a fazer sucesso, e ver o problema, era um problema de uso. era maior que um banco relacional, começaram a pesquisar alternativas e o primeiro resultado foi um banco de dados de grafos, o FlockDB. O FlockDB dava mais performance para operações de inserção, updates, já que não precisava das garantias ac de um banco relacional. Tinha operações extras para lidar com grafos, como navegar por nós e pelos vértices. E vocês têm que entender que carregar um grafo em memória pode ser pesado. Imagine sua própria conta, você tem dezenas de tweets, você segue dezenas de pessoas, pessoas tem dezenas de tweets. Cada um desses tweets tem contagens de likes, retweets, alguns tweets são replies para outros tweets e para calcular qual deles é mais relevante ou menos relevante, o ideal é ter toda essa estrutura em memória para conseguir navegar por toda essa informação. Agora pensa num banco de dados relacional como o mais ciclo. Como você resolveria esse problema? Qualquer solução que você imaginar em SQL pode ter certeza, já foi testado e sabemos que não vai escalar. Esse é um dos poucos cenários que podemos afirmar que um banco de dados relacional não é adequado. Segundo a página de GitHub do FlockDB, até 2010 o cluster deles armazenava mais de 13 bilhões de bordas e sustentava picos de tráfico de mais de 20 mil escritas por segundo e mais de 100 mil operações de leitura por segundo. Mas rapidamente chegou um ponto onde isso já não era mais suficiente, foi quando começaram a puxar esses dados para a HDFS e processando nesse novo projeto que chamaram de Castle Vary, um segundo projeto em Java para lidar com Big Graphs, grafos com bilhões de nozes e vértices. Foi a época que as celebridades gigantes começaram a viver no Twitter. Pense nível Kardashians. Voltando ao paper, a camada de armazenamento do Castle Vary dava acesso aos grafos via queries, pesquisas baseadas em vértices. Um SQL especializado em grafos. E em cima disso, um motor de recomendação computava sugestões de rootfollow, o WTF, ou quem seguir. Foi assim que nos primórdios do Twitter ganhamos sugestões de novas contas para poder seguir. Um dos objetivos desse projeto era ver se era possível manter os grafos em memória, em memória. A pergunta que eles não tinham resposta é, será que o crescimento do volume de dados desses grafos seria mais lento que a lei de Moore? Se for o preço de memória iria baratiar mais rápido do que os dados cresciam e daria para computar tudo em memória, que é mais simples do que ter que lidar com discos, particionamento, paging, cache e tudo mais. Como o paper diz, considere um grafo com 10 bilhões de bordas. Mesmo uma representação ingênua simples ocuparia algo na faixa de 80 GB que um único servidor consegue aguentar. Você já trabalhou com o servidor? Esse ano que tem 80 GB, se sim, diga nos comentários que empresa é pra todo mundo entender meu ponto. Uma das limitações, mesmo naquela época, é que essas sugestões de recomendação de quem seguir eram calculados em batch, uma vez por dia. Não era em tempo real como é hoje, ou seja, você só tinha recomendações novas uma vez por dia. De qualquer forma, via muita experimentação, muitos testes AB, eles chegaram em dois conceitos que usam até hoje que vale explicar. O primeiro se chama Círculo de Confiança e o segundo foi a adoção de Salsa, que é Stochastic Approach for Link Structure Analysis ou Analise de Estrutura de Links Usando Processos Estocásticos. Um exemplo de grafo que eu não mencionei, mas deveria ser óbvio, é a própria World Wide Web. Toda página na internet é um nó. As bordas vertas são linkas de uma página pra outra e no começo da web já se tinha um número grande de páginas, o suficiente pra se difícil de achar as coisas. Pense numa era antes de existir Google e a forma rudimentar que existia pra gente se achar era derivado do conceito antiquado de páginas amarelas. Se você for muito novo, talvez nunca tenha visto, mas antigamente, quando precisávamos achar um mecânico, um médico, qualquer serviço, tínhamos um livrão físico de páginas amarelas que era um diretório com todo o serviço da cidade. Cada cidade tinha um livro diferente. Imagine que novos negócios tinham dificuldade de ser encontrados porque precisava esperar uma nova edição sem pressa e distribuída pra todo mundo. Páginas de classificados nos jornais meio que cobriam um pouco essa diferença. Serviços como Yahoo, Excite, Laicos e outros serviam esse papel na web. Se eu criasse um site novo, pedia pra ser incluído num desses diretórios pra que as pessoas conseguissem me achar. Esse era o processo. Mas mesmo assim, imagine em restaurantes. Qual das centenas de restaurantes numa cidade grande como São Paulo seria relevante pra mim? Mesmo se tivesse a opção de filtrar por localização e tipo de cozinha, ainda assim eu tinha que manualmente tentar várias. Era bem trabalhoso. Portanto, resolver esse problema de relevância entre milhares de partes nas web era muito importante. E lá no meio por fim dos anos 90 surgiram pelo menos dois estilos diferentes de lidar com isso. A maioria de vocês que estudou o mínimo de grafos talvez esteja familiarizado com o famoso Paper de PageRank, literalmente ranking de páginas inventado pelos fundadores do Google Larry Page e Sergey Brin em 1996 quando ainda eram estudantes de Stanford. Procurem no YouTube. Tem dezenas de vídeos que explicam PageRank, mas a ideia básica é a seguinte. Começa pelo básico. O H de HTML é pra hyperlink. Todo mundo esquece isso, mas o mais importante numa página web é que a página de alguém contém o link pra sua página. Com mais sites diferentes tem links apontando pro seu site, mais relevante ele pode ser. No PageRank imaginamos um navegador aleatório, uma pessoa imaginária que vai clicando em páginas e nos links nessas páginas aleatoriamente. Qual a probabilidade de só navegando sem rumo alguém cair na sua página? Essa probabilidade é o seu score, sua pontuação. Se sua página é famosa, centenas de outras páginas contem links pra sua. A probabilidade desse navegador aleatório chegar na sua página é alta, portanto sua pontuação no ranking do PageRank é alta. Esse foi o algoritmo usado na primeira versão do Google, o site que matou todos os diretórios estilo páginas amarelas. Porque agora era possível fazer um programa que sai vasculhando todas as páginas da web, avaliando os hyperlinks entre eles e atribuindo pontuações nesse ranking. Daí quando fazemos uma pesquisa ele nos mostra primeiro as páginas mais bem pontuadas. É por isso que logo na primeira página os primeiros links costumam ser os mais corretos. Com isso eliminamos a necessidade de seres humanos ficarem manualmente classificando páginas em diretórios e atualizando a relevância na mão, que sempre foi demorado, trabalhoso, tedioso e com muita margem de erro. Essa talvez tenha sido a primeira categoria de trabalhadores da web que perderam seu trabalho pra um algoritmo e não será a última. Tudo que é feito manualmente e pode ser automatizado certamente será. Todo trabalho repetitivo é automatizável, sabemos isso desde o começo da web. Mas todo o algoritmo tem seu ponto fraco e rapidamente as pessoas começaram a notar que se o lance é esse número de links pra sua página, que tal criar um monte de sites fantasma que fazem links pra uma página que você quer tornar relevante. E assim surgindo os primeiros web farms, fazendas de servidores fake, e uma galera que vendia relevância pro seu site fazendo um monte de site falso linkar pra sua, seja criando sites besta em série ou invadindo sites dos outros pra colocar links. Isso aconteceu só lá no começo dos anos 2000, por isso o Google evoluiu e deixou de usar page rank e passou a usar por exemplo trust rank, que combate web spam identificando quem é site legítimo e quem não é e descontando pontuação. Ao longo dos anos, o Google sempre foi aprimorando esses algoritmos, não é um produto fixo. O limite do trabalho de agências web era ficar monitorando quando o Google lançava uma nova versão pra modificar as estratégias pra levar relevância pros seus clientes. Esses truques baratos não funcionam mais hoje. Mas não era só isso, mesmo sem spammer e gente maliciosa, as primeiras versões do page rank ainda sofriam de um problema grave. Pra ilustrar, veja meu conteúdo, meu canal e meu blog, que é de um domínio muito específico de assuntos de tecnologia pra programadores. Talvez eu faça um vídeo explicando em detalhes sobre o algoritmo do Twitter, exatamente o que eu tô fazendo agora. Pra programadores é relevante. Porém, imagine que um bosta como um Felipe Neto também faça um vídeo falando sobre o algoritmo do Twitter. Segundo o page rank, que é um ranking global, quer você goste dele ou não se pesquisar no Google de 1997, algoritmo do Twitter, esse vídeo do Felipe Neto iria aparecer na frente do meu. Mas a sua é a desvantagem de um algoritmo de ranking global como o page rank original. Mas como eu falei, as pesquisas em torno de grafos evoluíram rápido. Por exemplo, em 1999 surgiu o algoritmo hits, que significa hyperlink em dúcia de topic search de John Kleinberg. E hits opera num grafo direcionado onde páginas web representam nós e hyperlinks são bordas direcionadas. E até aqui igual o page rank. Mas o hits assinala duas pontuações pra cada página web. Uma pontuação se é um hub e outra pontuação se é uma autoridade. Hubs são páginas que linkam pra diversas outras páginas. Pense em um site do Estadão ou da Folha. São hubs de notícia que linkam pra trocentas outras páginas fora. Ou influências como um Nellipfeto que é um hub. Já autoridades são páginas que recebem muitos links de muitos hubs. Pense a página de uma receita federal em época de imposto de renda. Todo mundo faz link pra ela pra avisar que tá vencendo a data ou ensinando com um prensa ou imposto. Mas todo mundo pode ser ao mesmo tempo um hub e uma autoridade. Alguns são mais hub e alguns são mais autoridades num domínio específico e por isso duas pontuações distintas. Portanto o canal de um Nellipfeto da vida teria uma pontuação de hub muito maior do que de autoridade. Que é exatamente o perfil do influencer médio. Serve pra direcionar tráfico pra todo mundo. Mas ele mesmo não é autoridade de quase nada. Na caras como eu geram muito pouco tráfico pros outros mas todo mundo me menciona e linka pra mim em assuntos de um domínio específico de programação e tecnologia. Com isso minha pontuação de autoridade num vídeo sobre algoritmo de Twitter seria maior que do Nellip e eu apareceria no topo da pesquisa mesmo ele tendo muito mais tráfico e sendo muito mais famoso. Essa é a vantagem de algoritmos da família Hitz. Se comparado ao page rank original. Ele torna esse jogo bem mais justo do que uma pontuação única global e tenta melhorar a confusão entre influencer e autoridade. Só porque alguém tem muito seguidor de jeito nenhum o torna autoridade de coisa nenhuma. Porém se antes a forma ingenua era ter uma única pontuação global do page rank em Hitz teríamos sub grafos pra domínio ou tópico. Por exemplo programação gastronomia viagem cinema e cada vértice teria duas pontuações de hub ou autoridade por tópico. Portanto dá pra imaginar que é mais caro computar hits do que page rank. E claro o que temos hoje num Google é uma amálgama dessas técnicas e muitas outras que surgiram ao longo do tempo o que nos leva de volta a salsa. Salsa poderia ser considerado uma evolução da ideia de hits pra análise de links mas uma das ideias é não precisar navegar por todos os vértices e pontuar o grafo inteiro. Em vez disso ele implementa a ideia de andarilho aleatório ou random walker. É que nem pesquisa de opinião pública. Todo mundo tá acostumado a ver agências de fake news quer dizer imprensa tradicional fazendo pesquisas de opinião na rua pra dizer a metade dos brasileiros considera o governo atual uma bosta. Mas como eles sabem disso eu não lembro de ninguém me perguntando isso. Essa é a mágica da estatística. Podemos fazer sampling ou seja pesquisa só uma fração da população. Todo mundo mais aleatório forece sempre melhor. Uma coisa é perguntar pra 100 pessoas só de São Paulo. Outra coisa é perguntar pra 4 pessoas de cada estado. Assumindo que escolhemos bem esse grupo dentro de uma margem de erro podemos afirmar que a população em geral tem essa ou aquela opinião economizando ter que perguntar pros mais de 200 milhões de cidadãos do país. É uma puta economia de 200 milhões pra 100. Mesma coisa num grafo. Eu não preciso necessariamente andar o grafo inteiro. Eu posso escolher um grupo aleatoriamente em vez de usar o modo iterativo original do hits economizando muito processamento e tendo resultados que caem em margens de erro estatisticamente aceitáveis. Pra otimizar mais isso eles também reorganizam os nós e vértices num grafo bipartido que ajuda a separar os papéis de hubs e autoridades. Eu posso separar nossos nós em dois conjuntos distintos. Um com páginas que linkam pra outras páginas que são os hubs e outro grupo com páginas que recebem os linkas as autoridades. Não vale a pena detalhar hoje mas gravem esses dois conceitos de random walk e grafo bipartido pra estudar depois se tiverem interesse. Eu expliquei tudo isso porque o paper do Twitter menciona que eles desenvolveram um algoritmo de recomendação baseado em salsa que é um algoritmo de random walk na mesma família de hits originalmente desenvolvido pra ranking de pesquisa web. O algoritmo constrói um grafo bipartido de uma coleção de web sites de interesse. Hubs de um lado e autoridades de outro. E cada passo no salsa navega por duas bordas um pra frente e outro pra trás. E agora você sabe o que esse parágrafo do paper está dizendo. E ele continua nós adaptamos salsa pra recomendação de usuários da maneira mostrada nessa figura. O lado de hubs na esquerda é popular com o tal círculo de confiança do usuário. Usos usuários que ele escolheu manualmente seguir. Mas autoridades no lado direito são populados com usuários que esses hubs interagem. Depois que esse grafo bipartido é construído rodamos múltiplas iterações de salsa que assinala pontuações. Daí os vértices do lado direito são ranqueados por essa pontuação e tratados como recomendações de usuários. Os vértices do lado esquerdo também são ranqueados e interpretados como similaridade entre usuários. No princípio da homofilia que é só um nome bonito pra dizer que as pessoas com interesses parecidos tendem a ser a grupo a mais podemos apresentar esse resultado como recomendações que antigamente era uma sessão chamada similares a você. Resolvido a uma detalhadinha mais até aqui pra tentar explicar como foi o raciocínio por trás da primeira versão do motor de recomendação. Mas agora vou tentar andar um pouco mais rápido porque esse foi o sistema WTF que não existe mais. Mas a gente tem toda essa pesquisa experimentação durou pouco tempo apesar de ser um sistema bom que funcionou bem ele ainda não é o que conhecemos hoje. E em 2012 eles começaram a perseguir o tal projeto que é a Sovereign que eu mencionei antes. Pra ter recomendações ainda melhores eles queriam usar mais sinais dos logs de comportamento dos usuários comportamentos de likes retweets replies follow e tudo mais. Mas isso torna os metadados do grafo ainda maiores e a premissa da primeira versão que era tentar manter tudo em memória já não cabia mais. Daí veio a decisão de tentar usar Hadoop em particular a Pache Pig que pra quem não sabe é tipo um compilador que produz sequências de programas map reduce. Hadoop é uma plataforma de tecnologias criadas pra lidar com o problema de Big Data no auge da bolha da internet no começo dos anos 2000. Pra muita gente ainda hoje o conceito de Big Data não é muito palpável porque ninguém lida com isso no dia a dia. É de produto pra produto mas quando estamos gerando coisa de gigabytes por dia de dados terabytes por mês estamos em território de Big Data. O que você faz com banco de dados que tem centenas de terabytes. Data e chora porque não importa qual o bom seja seus índices e queries sua aplicação vai ficar absurdamente lenta só pelo tamanho. Nessa ordem de grandeza estamos falando que não é possível manter todos os dados numa única máquina mesmo sendo um servidor grande seja por limitação física ou de custo. Aí a única solução é a estratégia de Napoleão particionar e dividir esse tanto de dados em múltiplos servidores e daí conquistar. Por isso a base do Hadoop começa com o HDFS que é o Hadoop File System um sistema de arquivos distribuído. Agora o problema é o seguinte na faculdade você aprendeu a lidar com dados todos no mesmo lugar por exemplo digamos que você tem uma lista de produtos e preços e queremos achar o produto mais barato como que faz. Eles escolham um algoritmo qualquer de ordenação como um quick sort ou merge sorte da vida e pega o primeiro elemento que vai ser mais barato. Pra fazer isso precisamos iterar elemento a elemento da lista pelo menos uma vez. Mas e agora e citamos com cinco servidores e essa lista foi dividida por cinco. Intuitivamente poderíamos pensar beleza eu vou ter que repetir a ordenação uma vez em cada máquina. Daí no final eu vou ter uma lista parcial com os cinco produtos mais baratos achados em cada um dos cinco servidores daí eu faço uma última ordenação nessa lista e finalmente eu acho o mais barato. Cada um dos servidores vai rodar em um quinto do tempo do que se fosse a lista completa mas no final ainda tem pelo menos mais uma operação de ordenação extra e aí eu tenho resultado. Então seja tecnicamente um pouco mais lento rodar em vários servidores separados mas vai ser menos caro. Isso de rodar um algoritmo em pedaços dos dados mapear os resultados parciais e depois reduzir na resposta final é o que a grosso modo chamamos de map reduce que é parte fundamental de como fazer processamento de dados em radupe. Novamente eu não vou detalhar map reduce então anote para pesquisar mais depois. O map reduce ficou famoso porque depois do pade rank é como o Google conseguiu operar em larga escala e deixou todo mundo no chinelo no começo dos anos 2000. Hoje em dia é como toda grande empresa de tecnologia funciona mais 20 anos atrás essa forma de pensar em processamento de dados foi fundamental para possibilitar termos produtos que aceitam quantidades massivas de pessoas e transações literalmente ultrapassando a população total de diversos países. Tem mais gente no Twitter do que a população dos Estados Unidos metade da população do mundo está em produtos da meta. Por isso para a segunda versão do motor de recomendação o Twitter deixou o flockdb e wtf para trás e perseguir o tal projeto que é sovery para rodar em cima do hdfs do radupe usando map reduce mas eles tiveram vários problemas. Mesmo em 2012 já era sabido que grafos não performavam bem em radupe. Isso porque a maioria dos algoritmos conhecidos para processar grafos são iterativos. Lembra que eu falei para guardar essa informação? Para ilustrar pense nos diferentes algoritmos de ordenação que você aprendeu na faculdade. Alguns professores cometem o pecado de não explicar por que existem tantos e por que foram criados. Por exemplo o meu professor na faculdade mandou a gente decorar lá quicksort mergysort mas não explicou que mergysort foi criado pelo lendário von Neumann por problema de listas de dados que não cabiam na memória RAM da época. Então começavam gravados numa fita. Daí o algoritmo vai percorrendo os elementos da fita sem estourar a RAM e vai gravando ordenado numa segunda fita. Se tiver essa imagem de fitas na cabeça o mergysort finalmente faz sentido. Sem isso é um algoritmo que não faz sentido existir. Em 2012 ainda não existe o equivalente mergysort para grafos que não cabem em memória e precisam ficar divididos em parte de sonhos de hdfs distribuídos em múltiplos servidores. Hoje em dia já existe um suporte maior, pensam a paixa Spark com GraphX. Mas 10 anos atrás era um uso inédito ainda e o próprio paper descreve que eles estavam cientes dessas desvantagens. O paper menciona que o gargalo era na materialização dos vizinhos dos vertos que requeriam um enorme shuffling de dados que é pegar dados de diversos servidores para formar o grafo necessário para o processamento. Nessa parte do paper ele referencia outro paper, o Discovering Similar Users on Twitter, onde é explicado como exatamente eles conseguiram passar as barreiras de usar Hadoop, adaptando o algoritmo de PageRank, personalizado para ambiente distribuído. Como eu já expliquei, esse algoritmo é largamente utilizado para computar similaridade entre nós em um grafo, mas é iterativo e requer data shuffling frequente. Quando eu falo data shuffling, é tipo reembaralhar os dados. Pensa como isso é difícil quando seus dados estão distribuídos em diferentes servidores. Para resolver, os autores do paper propõem uma modificação ao algoritmo de PageRank personalizado chamado OnePass PPR, que reduz o número de iterações e minimiza esse reembaralhamento para cada iteração. Nesse caso, os dados são partitionados e baseados nos aides dos usuários e processados numa única passada sobre os dados usando uma combinação de pontuações local e global. O algoritmo também usa técnicas de caching para evitar recomputar resultados intermediários reduzindo o número de iterações totais. Além disso, os autores também propõem técnicas de compressão para reduzir o tamanho do grafo e possibilitar processamento mais rápido em sistemas distribuídos como Radoop. É muito detalhe para tentar explicar hoje, mas esse é o resumão desse outro paper que também eu recomendo darem uma olhada depois. É nele que se resolve o problema de conseguir dizer que quem segue um LeBron James tem alta probabilidade de seguir um Kobe Bryant, ou quem segue uma Lady Gaga costuma seguir uma Taylor Swift. E é onde nasce o framework que é utilizado até hoje e que eu vou retornar para explicar sobre a arquitetura de geração de candidatos e modelo de aprendizado baseado em regressão logística e similaridade de cosseno para achar usuários similares no Twitter. Voltando ao Castle Very, é só uma parte da solução, é quem dá acesso aos dados dos grafos em Radoop. A solução geral é chamada de realgraph, que é uma representação composta dos grafos de seguidores e grafos de interação induzidos através de logs de comportamento. Essa representação é construída de múltiplos pipelines de funcionalidades que são extraídas, limpas, transformadas e juntadas a partir dos logs screws. O realgraph é armazenado em hds e esse nome realgraph, vamos encontrar no código em escala do repositório que foi liberado, como podemos ver no VS Code. Ainda vez que vê realgraph, provavelmente é isso. Mas esse segundo motor de recomendação ainda tinha um grande defeito, só servia para recomendar usuários similares para você seguir e só atualizava poucas vezes ao dia, não era nem perto de ser tempo real. Funcionava, mas ainda não é a aba para você que tem no Twitter hoje. Pulamos do WTF de 2010 para o Castle Very Radoop Realgraph de 2012 e agora saltamos para 2014 e um novo paper que descreve algumas novas estratégias para particionar e organizar esses grafos. Como vocês podem ver, não era um problema trivial de resolver. No caso de Twitter e até em sistemas de mensageria em geral, existe uma particularidade nos dados. Você nunca precisa do grafo inteiro dos usuários. Por exemplo, pouca gente está interessado em ver tweets de mais de um ou dois dias atrás das pessoas que segue. Então mesmo que seja o influencer favorito do Twitter todos os dias, o que seria um volume grande de tweets para o grafo, podemos limitar uma janela de um ou dois dias para todo mundo lidar com um subgrafo muito menor. Pensando assim, não precisamos lidar com operações como deletar um tweet do grafo, por exemplo. Basta pensar em expiração. É mais ou menos o mesmo pensamento por trás de caches como mencash ou redis da vida que usamos hoje. Não pense em como apagar dados velhos, estabeleça uma janela de tempo e deixe expirar sozinho, que é uma boa prática de qualquer tipo de caching. Não só isso, também não precisamos guardar o timestamp exato de cada tweet no grafo. Isso vem da observação que a maioria das interações, como likes, tweets, replies, costumam acontecer muito próximo do momento que o tweet foi criado. Pra maioria de nós é interessante saber que o tweet foi postado uma hora atrás, cinco horas atrás, um dia atrás, mas não exatamente quando. É menos um dado pra ter em memória mais uma economia. Pensando em tudo o que eles aprenderam até agora, iniciariam um terceiro motor de recomendação em cima de outro projeto que eles também tornariam open source, chamado graphjet. Ele mantém um grafo bipartido que registra interações de usuários a tweets durante as mais recentes n horas com uma API super simples só com as operações que eles sabiam que realmente precisam, sem fazer over engine ini de criar um banco de dados de grafos genérico demais. E finalmente tinha uma experiência suficiente pra conseguir especializar a solução. O resto do paper entra em detalhe sobre como foi a estratégia de mapeamento de IDs, estratégia de gerenciamento de memória pra balancear reembaralhamento e distribuição dos dados e em resumo eles conseguiram chegar no modelo onde só o processamento pra encontrar usuários e tweets recomendados ficou na casa abaixo dos 10ms, que é perto do ideal pra conseguir ter resultados em real time, onde a maior parte dos gargáutos era a comunicação com os diversos sistemas de filas, como a paixa e cáfica que eles usam até hoje. Esse paper é bem bacana e eu recomendo ler com muita calma e anotando cada termo que não conhecem pra pesquisar depois, ele descreve as 3 gerações de motores de recomendação dentro do twitter até 2016. Eu não parei pra ver se tem papers mais novos que continuam essa história, mas graças a isso o código liberado que tem vários trechos que mencionam real graph ou graph jet agora deve fazer mais sentido, entendeu? Sem esse tipo de pesquisa só lendo o código liberado ninguém nunca ia entender do que se trata, precisa do contexto histórico. E isso é só a parte do motor de recomendação, existem diversas outras funcionalidades no twitter, por exemplo o motor de pesquisa que é quando você faz pesquisas por palavras chave e ele retorna tweets com esse conteúdo que nem uma pesquisa do google. Na figura da solução do quesa overy que puxa dados do hdfs que vieram no flock db pra cima tem fetchers que mandam pra um serviço chamado blander. Blander foi um front change escrito em Java e em cima do net que é a grosso modo equivalente ao node js pra java. Por baixo usa o lendário apache lucini como motor de procura. E lucini é feito em java e a gente usa faz anos, toda vez que um site tiver um campo de pesquisa e os resultados forem bons pode ter certeza que tem lucini por trás. O produto mais famoso que é construído em cima do lucini é o elastic, também conhecido como elastic search ou o solar da própria apache. Full text search, coerção ortográfica, autocomplete, tudo isso já é problema resolvido por lucini, solar ou elastic. Eu só encontrei um post no blog de engenharia do twitter e outro post de um ex programador que trabalhou no twitter por volta de 2012, mas além do blander existia um outro sistema relacionado a parte de pesquisa em tempo real que é diferente do sistema de recomendação chamado early bird. E eu menciono isso porque early bird é outro termo que aparece bastante no código que foi liberado recentemente, então agora você sabe. E aproveitando que eu comecei a falar de projetos open source do twitter, na parte do paper onde ele explica o desenvolvimento da solução graphjet, eu falei que tem uma sessão que fala da estratégia de IDIS. Isso gerou outro projeto open source chamado Snowflake, que eu não conhecia também, mas em dados distribuídos, IDIS únicos é um problema. Por exemplo, numa tabela de banco de dados mais sico rodando na sua máquina você coloca um inteiro auto incremental e esquece. Cada nova linha vai ganhando um ID como 1, 2, 3, 10, 11, 12, 1, 1, 1 e 2 e assim por diante. Os IDIS incrementais tem diversos problemas, um deles é segurança, mas o outro é conseguir particionar esses IDIS. E quando eu tenho um banco de dados distribuído em dois servidores, eu vou só dizer que no primeiro servidor vai de 1 a 1 milhão, no segundo servidor vai de 1 milhão a 2 milhão, isso não escala, se você for mais esperto vai lembrar. Ah, podemos usar coisas como UID ou GUID da Microsoft, IDIS de 64 bits ou 128 bits aleatórios, ou até espaços menores, de 48 bits como MAC address de placas de rede. Quanto maior o ID, mais espaço vai ocupar em disco e quando carregar em memória. E se vamos desperdiçar esse espaço, o ideal é que não seja aleatório e sirva algum propósito. Daí temos IDIS semi-aleatórios, no caso da estratégia do Snowflake, o ID é um número aleatório pré fixado com o timestamp, um identificador do servidor onde está e uma sequência numérica aleatória. E dessa forma eu consigo agrupar tweets que foram criados num determinado período de tempo e eu tenho a localização no cluster de servidores, mas eu também sei que não vai ter colisão desse ID com outro ID em outro servidor, quer dizer, até pode ter colisão mas a probabilidade é hiperbaixa, perto de zero na prática. Eu menciono isso porque se procurar no VS Code olha só, Snowflake aparece em diversos lugares do código e achamos trechos que usam a função timefromidopt, onde passamos um ID de Snowflake e ele retorna um timestamp, daí podemos fazer operações como ver se foi criado antes ou depois de uma determinada data. Se estudar bancos de dados distribuídos, vai ver como existem diferentes estratégias de IDs únicos para diferentes necessidades. Um banco de dados como Cassandra usa outra estratégia chamada TimeUuID, que é muito similar, já que também tem um componente de timestamp embutido no ID, o que facilita fazer queries de períodos de tempo e análise de time series, que são séries de tempo e processamento de eventos. Um DynamoDB da Amazon tem um identificador único chamado de chave de partição, que é usado para particionar dados em múltiplos servidores no cluster. A chave de partição é combinado com uma chave de ordenação, que é usado para ordenar os dados dentro de uma partição. Sem fugir demais do assunto, mas já que eu falei disso, eu preciso pelo menos explicar outro conceito. Em bancos de dados distribuídos, um problema seria se um servidor fica mais cheio que outros servidores. Idealmente queremos balancear o volume de dados entre cada servidor, e uma das formas de fazer isso é o que chamamos de consistente hashing. Esse algoritmo mapeia chaves de partição, como o snowflake do Twitter ou TimeUuID do Cassandra, em nós virtuais num anel, um ring. Cada nó virtual representa um servidor físico no cluster. À medida que novos nó são adicionados ou removidos do cluster, os ranges de partição são redistribuídos por todos os nóis virtuais, garantindo distribuição uniforme das chaves de partição. E novamente, é um problema bem conhecido e bem resolvido nos mais diversos bancos de dados distribuídos. Eu lembro quando liamos papers da Amazon sobre isso, lá por 2008 ou 2009, mas para um estudante de ciência da computação. Hoje continua sendo um problema fascinante de entender, já que o conceito é bem simples. Um ID é muito mais do que só uma etiqueta qualquer. Ela serve para facilitar pesquisas e para facilitar balanceamento de sistemas distribuídos. E para terminar essa parte de open source, tem outros dois projetos que são mencionados que eu acho importante explicar. O primeiro se chama thrift, e o segundo é o protocol buffers ou protobuffs. Acho que muitos de vocês já devem ter ouvido falar pelo menos de protobuffs. Bem a grosso modo, pense em microserviços ou endpoints de APIs hoje. A maioria de vocês deve estar acostumado a ter endpoints que se comuniquem ou simplesmente cospem dados em formato JSON. É o Hello World de API em todo tutorial e curso, só que JSON não é a única forma e para muitos casos é a pior opção. JSON é um formato excelente para endpoints de APIs que seres humanos como eu ou você vamos manipular. É indicado para coisas como APIs de e-commerce, onde queremos puxar coisas como principais ofertas para mostrar no nosso blog. Mas para comunicação entre sistemas, por exemplo, o back-end do e-commerce conversar com meios de pagamento ou logística, ou mesmo para comunicação entre microserviços dentro da mesma empresa, é um dos protocolos mais porcaria que existem. Como um exemplo bem, mas bem tosco, imagine esse pequeno JSON de um produto de e-commerce. Quem já trabalhou com e-commerce sabe que é bem maior do que isso, mas para ilustrar aqui no vídeo é suficiente. JSON é um protocolo texto, onde o esquema vai embutido junto com cada registro. Como eu falei, isso facilita para pessoas comuns conseguirem ler sem muita dificuldade. Agora, imagine um esquema de protobuf sendo assim. Note, ele define exatamente que tipo de dados, portanto, qual o tamanho exato de cada campo de dados. Dessa forma, eu não preciso de coisas como aspas, vírgulas, nada disso, eu posso concatenar um valor atrás do outro, porque o tamanho vai ser fixo. E no final, o mesmo JSON convertido em protobuf vai gerar um linguição binário, quem é que essa decimal seria assim? É isso que a nova API com protobuf vai cuspir. Como eu disse antes, ao contrário do JSON, esse formato é 100% hostil para um ser humano conseguir ler. Porém, é 100% perfeito para um computador ler. A fase de converter o JSON numa estrutura de dados é muito mais simples, e um protobuf é muito mais próximo de uma struct de C, só que mais genérico para funcionar em qualquer linguagem. O JSON do exemplo é um string que pesa quase 200 bytes. Já esse linguição que contém exatamente os mesmos dados pesa menos de 80 bytes. Mesmo nesse exemplo supertosco, estamos falando de uma economia de 2.5 vezes na transferência de rede, sem contar a economia de processamento. Lembra que eu falei que o Twitter lida com milhões de tweets todo dia? Imagine se internamente, em todo sistema de pesquisa, bancos de dados distribuídos, sistema de recomendação, filas de processamento, ficasse trafegando tudo em JSON, terabytes e terabytes de bandas sendo desperdiçados. Por isso, o Twitter adotou o equivalente a protobufs do Google, o drift do Facebook. Drift e protobuf é quase a mesma coisa. A única pergunta que você poderia ter é por que o Twitter não usou o protobuf do Google então, em vez de reinventar a roda? E de fato, protobufs foram criados antes, sendo usados dentro do Google desde pelo menos 2001, mas só foi lançado publicamente em 2008 e nesse meio tempo surgiu o Facebook que teve problemas similares para resolver e inventar um drift que lançaram em abril de 2007. Por isso, o Twitter só tinha a opção do drift em 2007. Mas para anar os projetos, especialmente que interagem com componentes como o PyTorch ou o TensorFlow, que também veio do Google, eles passaram a usar protobufs e você vai achar os dois no código. Drift e protobuf são protocolos de serialização de dados. Esse tipo de protocolo é ideal para comunicação entre microserviços, só que diferente de JSON ou XML, a forma de fazer essa comunicação não é via HTTP, como num web service e envia RPC ou Remote Procedure Call, que formaliza como um serviço chama uma função ou procedura de outro serviço. No caso do Google, eles tem o GRPC, que é um framework de RPC que tem como objetivo prover comunicação eficiente e escalável entre serviços na rede. Twitter não usou o GRPC. Finagall é o equivalente de GRPC feito pelo Twitter que usa drift para a serialização de dados em vez de protobufs. Tem os mesmos objetivos de lidar com comunicação de autofruput, baixa latência e na rede. E o Finagall é implementado em cima de net, também usando muita funcionalidade de programação acíncrona com futures de escala, que é o equivalente promises de JavaScript de hoje. Novamente, o Twitter Finagall foi lançado em 2011, o GRPC do Google só sairia por volta de 2015 e o Twitter estava tendo que inventar tecnologias que não existiam ainda e por isso é um case fascinante. Mas claro, Google sendo um nome muito maior e mais reconhecido que Twitter, matou os projetos drift e Finagall. Num projeto moderno você deveria escolher protobufs, GRPC ou alternativas mais modernas. Hoje o drift ainda existe como Apache Drift, mas da Apache temos outras alternativas como Apache Avro, que foi criado mais para o cenário de streaming de dados serializados, ou seja, quando você abre um streaming de um serviço para outro serviço e trafega registros serializados como Avro entre eles. Apache é um nome muito reconhecido para quem é de Java e também no espaço de Data Sciences e Machine Learning. Quem lida com Kafka e é de Java deve ter usado Avro como opção de serialização para jobs em vez de protobufs ou JSON. JSON é mais para linguagens menores, como JavaScript ou mesmo Ruby e PHP. Protobufs são mais usados em soluções que envolvam tecnologias do Google, como em Android, e de qualquer forma vale a pena estudar todas. Então acho que o mundo acaba em JSON ou XML, tem coisa muito melhor e muito mais eficiente que é usado por sistemas realmente de grande volume, para e-commerce da esquina de fato o tanto faz. E falando em Java, acho que vale a pena relembrar a parte inicial dessa história, o que aconteceu justamente em 2007 a 2010. Essa era a época que eu era mais participativa na comunidade Ruby on Rails e claro, o Twitter era um dos exemplos de maior sucesso de Rails, mas aí vem a notícia bombástica, o Twitter declarou que estava migrando de Ruby pra escala e daí veio o meme que não morre de Rails na escala. A forma como o Alex Spenney, um dos principais desenvolvedores do Twitter, descreveu ou melhor, não descreveu os problemas que estavam enfrentando, tudo que eu expliquei até aqui, causou má interpretação e muita gente falando muita merda. Eu conversei com outro dos principais desenvolvedores daquela época, Blaine Cook em junho de 2008 e mesmo naquela época eu também não tinha uma visão completa dos acontecimentos. Depois leio a entrevista original que está no meu blog. Por exemplo, da forma como o Alex Spenney falou, deu impressão que ele estava dizendo que Ruby on Rails não suportava conectar com múltiplos bancos de dados, que não era verdade. Sim, não tinha uma chavinha fácil, mas era basicamente uma linha extra que precisava ser colocada na classe pie de todos os activi records e dava pra selecionar onde fazer a conexão. Eu e vários outros descrevevam os tutoriais de como fazer isso. Mas a verdade é que o problema não era esse. Ninguém de fora do Twitter tinha muita noção do volume de dados que eles estavam tendo que lidar. E isso era 2007. Lembre-se, a Amazon AWS tinha lançado não fazia um ano. Produtos que hoje são a Roys com feijão, como SQS, DynamoDB, Route 53, nem tinham sido lançados ainda. Firebase ainda não existia. Porra, coisas como Redis ainda não tinha sido inventado. GitHub ainda não tinha sido inventado. A verdade é que particionar dados, organizar o MySQL em Sharks e conseguir fazer marketing e arquitetura coesa em cima disso não era trivial. E o Twitter estava numa fase de crescimento rápido. Não é que o Ruby on Rails não escalava pra escala do Twitter. Nada em Python escalaria. Nada em PHP escalaria. Node.js ainda não tinha sido inventado também, mas também não escalaria. Entenda, tinha poucas opções. Possivelmente a opção mais completa teria sido começar a implementar um banco de dados de grafos em cima de um Erlang, algo inspirado do banco de dados Minigia que vem no ATP de Erlang. Por exemplo, vocês já viram Erlang? Hoje ele ainda é pouco usado e em 2007 menos ainda. É verdade que grandes sistemas de telecomunicações já usavam fazia anos, mas eram um salto bem grande pra uma tech startup tomar. Quais as alternativas? Eles precisavam de uma linguagem pra escrever componentes como sistemas de fila, coisas no nível de Kafka. E lembrando que Kafka só seria lançado em 2011. Coisas no nível de Cassandra, que também só seria liberada publicamente pelo Facebook em 2008. Neo4j talvez fosse o componente mais próximo de se encaixar nos proeirões do Twitter, mas era super novo, tinha sido lançado em 2007. Entendem? Imagina você sendo desenvolvedor do Twitter, vendo milhares, centenas de milhares de tweets sendo postados todos os dias e tendo que lidar com grafos de usuários e tweets, sendo que algoritmos de grafos nunca tinham sido testados com volumes desse tipo. Numa época, onde toda tecnologia de cloud, no-sql e devops ainda não tinha sido inventado, onde linguagens mais comuns como Ruby, Python, PHP, JavaScript não estavam ajudando e programar em C++ ou Java parecia ser um passo pra trás. Elixir não tinha sido inventado ainda, Goa não tinha sido lançado ainda, Rust não tinha sido inventado ainda, LLVM ainda era uma grande novidade, já que a Apple tava só começando a mover suas ferramentas e linguagens de GCC pra Cileng, como o conto no episódio de Apple e GPL, mas havia uma única linguagem que talvez pudesse fazer sentido, a linguagem Scala, que tinha sido lançada em 2001 pelo visionário Martin Odersky. O Odersky é um desses gênios incomprendidos, tanto que tirando o povo da área de Java, eu duvido que qualquer um aqui já tenha ouvido falar dele, mas deveria. O cara é PhD de ciência da computação pela ITH de Zurich e trabalhou sob a supervisão de ninguém menos que Niklaus Wirff, o inventor de várias linguagens como o Modulador e Pascal, e o autor do livro de estrutura de dados e algoritmos que eu usei na faculdade. O Odersky participou e fez contribuições no mundo Java desde a versão 1.1, tendo trabalhado no compilador, na implementação de generics e algumas linguagens experimentais como a pizza e funnel, que influenciaram o design do Java. Muito antes de programação funcional ficar hypezinho de hipster sem graça como é hoje, no começo do século ele já vislumbrava uma linguagem que destilasse o melhor da orientação a objetos como os atrates, que são como protocolos de Objective C em vez de herança excessiva, mesclado com conceitos de programação funcional. Scala já tinha Higher Order Functions, já usava tipo opcional evitando os famosos no PointerException, Pattern Matching e tudo que se vê em linguagens mais modernas de hoje. Então era de fato a linguagem mais moderna que se podia escolher em 2007, mas rodando em cima da JVM que era a virtual machine mais madura que existia, dava pra criar aplicações que só seriam possíveis em C++ com uma fração da dor de cabeça. Pra isso custaria um pouco mais caro em uso de recursos do que se fosse puramente nativo, mas a JVM naquele ponto oferecia um ecossistema que ninguém mais tinha. E o objetivo do uso de Scala não era pra fazer front endzinho, era pra fazer os diversos componentes que eu expliquei que ainda não existiam. Começou com o projeto open source Starling, que é mais ou menos um mini-cáfrica alguns anos antes do Cáfrica, inventaram o Real Graph e GraphJet antes de um banco de dados de graps madura existir. Utilizaram coisas como Hadoop, que também faz parte do ecossistema Java. Nada disso teria sido possível sem C ou em Java ou em C++. Hoje em dia eles teriam muito mais opções, como o próprio Elixir, Go ou Rust Se quisessem arriscar linguagens mais exóticas, teriam TheNims, Zig, Talvez Swift, Kotlin, mais de novo, era 2006 pra 2007 e o relógio estava correndo. Em retrospecto foi uma decisão muito acertada, e é por isso que o código que foi liberado é dois terços Scala e Java e muito das pesquisas que o Twitter fez. Junto com trabalhos do Facebook, Google, Apple, Amazon, Geror, o atual ecossistema de linguagens, ferramentas e plataformas pra lidar com Big Data, Data Science, Machine Learning, culminando com Deep Learning e a atual geração de inteligência artificial que está todo mundo babando. Mas foi assim que as coisas começaram no início do século. Agora que temos uma noção melhor do contexto histórico dos projetos e papers de pesquisas gerados, podemos começar a entender melhor o código liberado. Esse código é um conjunto de diversos subprojetos. Se abrirmos o primeiro ritmo na raiz do repositorio, eles descreveram o que são cada um desses projetos. Por exemplo, SimClusterN é o serviço que retorna candidatos de recomendações de tweets dado em badgings de um cluster de similaridade. Esse serviço usa o algoritmo de similaridade aproximado de cossendo. Já vou explicar isso. Segundo o ritmo, um job constrói uma apéamento entre SimClusters e Tweets. O job grava os top 400 Tweets de um SimCluster e os tops sem SimClusters pra um tweet. Pontuação de favoritos e pontuação de seguidores são dois tipos de pontuação que um tweet pode ter. E os tops sem SimCluster baseados nessas pontuações é que chamam de tweet SimCluster embedding. A similaridade de cossendo entre dois embeddings apresenta o nível de relevância de dois tweets no espaço de SimClusters. A pontuação varia de 0 a 1. A alta pontuação de cossendo de maior que 0.7 significa que os usuários que gostam de dois desses tweets compartilham os mesmos SimClusters. Vamos lá. Similaridade de cossendo é um conceito de ause bralinear pra medir quão similar são dois vetores em termos de direção e magnitude. Eu gosto desse conceito porque eu expliquei isso desde pelo menos 2015 até minha última palestra pública em 2019. Quem comeu a minha palestra restrição igual a inovação vai se lembrar disso e eu vou aproveitar pra usar os slides que eu usava nessa palestra. Comece visualizando a página inicial do Google o campo onde se digita palavras chaves. Podemos digitar Apple e recebemos páginas indexadas da web que tem essa palavra. Um amador que acabou de aprender SQL poderia pensar que é alguma coisa parecida com Select, URL, From Page, Square Text, Like, % Apple por cento. Essa é a solução que faríamos no dos anos 90 antes de Larry Page, Sergey Brin inventar em Page Rank, antes de John Kleinberg inventar hits e essa linha de SQL de fato vai achar todas as partes onde aparece a palavra Apple, mas não vai saber como ordenar. Qual delas é mais relevante e deveria estar no topo da lista. Pra saber relevância precisamos de álgebra linear. Tudo depende de como você enxerga o que é uma página web. Todo mundo enxerga um documento, uma coleção de palavras que é o texto desse documento. Digamos que nosso banco de dados só tem três documentos indexados como nesse slide. Pra simplificar bastante, vamos assumir que nosso vocabulário é bem limitado e os documentos só tem as palavras Apple e banana. O documento 1 tem quatro apples e uma banana, o documento 2 tem três bananas e um Apple e o documento 3 tem cinco bananas e nenhum Apple. Podemos representar esses documentos como vetores como nesse slide. Próximo passo, criamos um índice que é uma lista que diz em quais documentos aparece a palavra Apple e quais aparece banana. Pense em algo como índice do seu banco de dados MySQL ou Postgres ou SQL Server da vida. Agora vamos digitar a palavra Apple no campo de pesquisa desse nosso Google de mentira. O truque começa a representar essa pesquisa, essa quele também como um vetor, e no caso Apple sendo 1 e banana sendo 0. É como se fosse um documento também. Pelo índice podemos descartar o documento 3 já que Apple não aparece nenhuma vez, mas ainda temos o documento 1 e o documento 2. Qual deles é o mais relevante para aparecer no topo da lista? E de bater o olho sabemos que é o documento 1 porque aparece a palavra Apple quatro vezes e no documento 2 aparece uma vez. Mas como o algoritmo conseguiria computar isso? Pra isso desenhamos esses vetores num espaço vetorial, um vector space. E no caso só temos duas dimensões porque só temos duas palavras no nosso vocabulário. Apple, eixo x, banana e o eixo y. Então o vetor que representa o documento 1 seria assim, quatro unidades pra direita no eixo x e uma unidade pra cima no eixo y. Já o documento 2 seria só uma unidade pra direita no eixo x, mas três unidades pra cima no eixo y. Finalmente o vetor da query é só uma unidade pra direita no eixo x, só uma palavra Apple, entenderam? O lance com vetores é que agora temos ângulos entre o vetor de pesquisa e os vetores de documentos. E pra calcular a similaridade de cc, não usamos dot product que é produto escalar. Funciona assim, vamos multiplicar Apple com Apple, um banana e somar os dois. Então um vetor de pesquisa é um em zero, um Apple e zero bananas. E vetor do documento 1 é quatro Apple e uma banana. Então a conta seria uma vezes quatro, mais zero vezes um que dá quatro. Esse é o produto escalar. Daí fazemos a mesma coisa entre o vetor de pesquisa de novo e o vetor do documento 2. Então o produto escalar vai ser um Apple vezes um Apple, mais zero banana dando um. Como o produto escalar quatro, do documento 1 é maior que o 1 do documento 2, sabemos que o documento 1 tem uma similaridade com a pesquisa e portanto é o mais relevante pra aparecer no topo da lista. Esse exemplo é super trivial porque só usamos duas palavras, então temos só duas dimensões. Mas vamos complicar, e se tivéssemos um novo documento indexado que tem a palavra coco, o vetor dele poderia ser um Apple, zero banana e dois cocos. Todos os outros vetores precisam ser atualizados pra contê coco, mas no caso de zero cocos, então não muda nada. Pra representar o espaço vetor, ele é o precisamos de uma nova dimensão Z. O vetor desse novo documento seria desenhado com uma unidade pra direita no eixo X e duas unidades pra baixo no eixo Z. E podemos calcular o produto escalar entre a pesquisa e esse novo documento também. Em geometria a gente consegue desenhar no eixo Z três dimensões. Mas claro, o vocabulário de uma língua como o português é muito mais que só três palavras. No dicionário como o aurelio, vai ter mais de 500 mil palavras, portanto podemos ter potencialmente 500 mil dimensões nesse espaço vetorial. Na realidade vai ser a quantidade de palavras únicas que tem no documento ou parte na web de verdade. O script desse documento, por exemplo, tem mais de 3 mil palavras únicas, é um vetor de 3 mil dimensões. Felizmente, o escalar é uma operação muito rápida de se calcular em qualquer computador moderno, só somas e multiplicações simples. E é assim que funciona, a base de todo tipo de algoritmo de relevância e recomendação. É isso que tem por baixo da biblioteca Lucini que eu falei antes. E se olhar a documentação de um elástico, vai ver que menciona a VSM ou Vector Space Model, que é esse gráfico que eu vim mostrando muito maior do que os três do exemplo. E é isso que esse projeto Sim Cluster vai fazer, só que com vetores de usuários e tweets. No caso do Twitter, esse primeiro projeto se chama Sim Cluster N. Sim Cluster é Similarity Cluster e esse N provavelmente significa Approximately nearest neighbor, ou o vizinho mais próximo aproximado. Em Machine Learning e Data Mining, pesquisa de vizinho mais próximo é o problema de encontrar o ponto de dado mais próximo em um espaço de muitas dimensões dado um certo ponto de pesquisa onde soluções exatas são enviáveis pelo tamanho do conjunto de dados. Algoritmos de vizinho próximo aproximado como Locality Sense TV Hatching ou LSH e métodos baseados em árvore como KDTrees oferecem uma forma eficiente de resolver esse problema fazendo um trade-off de algum grau de precisão por uma melhoria significativa em performance. Então é mais uma otimização do Twitter para ganhar performance, mais ou menos como o Random Walk do algoritmo de salsa que eu expliquei antes. Nesse subprojeto o conceito importante são Embedding Spaces que tem com o objetivo responder a pergunta quais tweets e usuários são similares aos meus interesses. Embedding funciona gerando representações numéricas dos interesses dos usuários e conteúdos de tweet. Daí podemos calcular a similaridade entre quaisquer dois usuários, tweets ou pares de usuários e tweets e espaço de embedding. Justamente um dos espaços de embeddings mais úteis do Twitter são SimClusters. SimClusters descobrem comunidades ancoradas por um grupo de usuários e influentes usando um algoritmo customizado de factorização de matrizes. Existem uma 145 mil comunidades que são atualizadas a cada três semanas. Usuários e tweets são representados em espaço de comunidades e pertencem a múltiplas comunidades. Elas podem ter algumas milhares até milhões de usuários. Segundo o blog de engenharia essa imagem representa algumas das maiores comunidades. Sem entrar em muito mais detalhes vamos bater o olho em alguns outros projetos descritos naquele arquivo Rhythm. Veja um aqui chamado Real Graph. Eu falei de Real Graph antes. Veja esse tweet cred que é o algoritmo de PageRank para calcular a reputação dos usuários. Temos esse Reacos Injector que é descrito como o lançador de eventos em Stream para construir os streams de entrada para serviços baseados em GraphJet. Já sabemos o que é GraphJet mas Reacos eu acho que significa recomendações. Então é um Injetor de recomendações para GraphJet. Pulando lá para baixo temos esse Light Ranker que é um ranqueador mais leve usado pelo índice de pesquisa o tal Early Bird que eu mencionei antes que é um projeto legado e parece que ainda é usado o componente construído em cima de a paixa Lucini. O Heavy Ranger está no outro repositório de Algorithm ML que o blog post do Steven que falei lá no começo menciona e segundo o Rhythm é o responsável pela aba para você e tem aquela lista de pesos. Nesse ponto da cadeia depois da fase de Candidate Sourcing temos uns 1500 candidatos que podem ser relevantes. Ponto ação serve para prever a relevância de cada tweet candidato e é o sinal para arranquear tweets na timeline. Nesse estágio todos os candidatos são tratados igualmente e o ranqueamento sai de uma rede neural de uns 48 milhões de parâmetros que é continuamente treinada com interações de tweets para otimizar o engajamento positivo que são likes, replies e tweets segundo o blog de engenharia do Twitter. Sendo bem honesto eu não tenho certeza de como exatamente isso funciona ainda. Olha o diagrama que tem no próprio arquivo Rhythm que lista os projetos. Os dados vêm dessas intituladas grafos social, engajamento de tweets e dados dos usuários. Daí temos features como o GraphJet, SimClusters, Tweet Him que é outro projeto no repositório de Algorithm ML e serve para pré treinar os tais em badgings. Daí temos Real Graph, Tweet Pcred, e Trust and Safety que parece ser um projeto de TensorFlow para treinar modelos para filtrar coisas como conteúdo pornográfico, adulto ou o que ele considera tóxico. Não sabemos exatamente por que não foi liberado o modelo e nem os dados de treino. É um projeto bem pequeno e quem entende de tem o sort flow poderia dizer um pouco mais mas não tem muito para se ver. Continuando temos essa coluna que ele chama de Home Mixer. Daí temos três fases. Um que ele chama de Candidate Sourcing que é literalmente escolher os candidatos, candidatos de recomendação e esses candidatos passam pelo tal Heavy Ranker que parece ser o modelo de Machine Learning via PyTorch. O resultado disso passa pela etapa final de heurísticas e filtros. Então para se chegar nos tweets recomendados na aba para você na etapa de sourcing já acontece diversos filtros para selecionar os candidatos de tweets que você pode gostar de ver. Esses filtros passam pelos projetos de SimCluster, Real Graph, TNS, Tweet Pcred, os GraphJets e são mesclados pelo projeto CR Mixer. Isso vale comentar. Os tweets gerados mais ou menos são metade de tweets gerados pelas pessoas que você segue o que ele chama de In Network e a outra metade de pessoas que você não segue que ele chama de Out of Network. E quem puxa isso é esse projeto CR Mixer. Dado esse conjunto inicial de tweets, In e Out of Network aí vem a parte de recomendação propriamente dita usando o Light Ranker que é o projeto Early Bird e daí o Heavy Ranker. Mas antes de mandar para você ainda passa por um Pipeline de filtros. Pelo projeto Home Mixer propriamente dito, esse Visibility Filters que é onde tá implementado a política de censura do Twitter. E por último tem esse projeto Naave que parece mais recente escrito em Rust mas é super pequeno e não faz muita coisa. Parece mais um adaptador para falar com os componentes de TensorFlow e PyTorch e um tal de Onks que eu não conhecia mas parece que significa Open Neural Network Exchange que é um formato open source para representar modelos de Machine Learning. Foi originalmente desenvolvido pela Microsoft e Facebook em 2017 para facilitar interoperabilidade entre diferentes frameworks de Deep Learning, por exemplo, entre PyTorch e TensorFlow. Mas realmente eu não sei como ele é usado nesse projeto ainda. Se tiver interesse em saber mais, cada um desses subprojetos tem um Rhythm também. Então eu recomendo que comecem por eles. Mas como eu disse no começo tenta rodar Base ou Build em qualquer diretório ele vai reclamar da falta de um arquivo de Workspace. Então até o Twitter liberar isso ou alguém se dá o trabalho de gerar na mão não temos como compilar e nem de tentar rodar nada. Por isso que é meio chato porque mesmo lendo essas coisas todas na documentação e no próprio código não temos como testar.
