 E no caso desse código do Twitter, sim, eu encontrei que ele usa coisas como grafos, regressão logística e similaridade de cosseno. Mas eu to me adiantando. Vamos voltar ao paper do GraphJet de 2016. Um problema que não é trivial de resolver e não tem uma solução única porque depende do comportamento dos usuários e dos dados disponíveis é recomendação, seja recomendação de produtos no e-commerce, recomendações de leitura nos sites de notícias ou recomendação de tweets num Twitter. Como usuários seguem usuários, naturalmente temos grafos, em particular grafos bipartidos, com usuários de um lado e tweets de outro. Os esforços de recomendação começaram no Twitter por volta de 2010, lembrando que o Twitter começou a operar em março de 2006. Eu mesmo queria em minha conta só um ano depois, em abril de 2007, só fui usar durante a RailsConf 2008 em Portland, onde eu via todo mundo se cadastrando pra gente compartilhar onde ia ser o próximo Happy Hour. Lembrando que ainda não existia Messenger do Facebook, não existia WhatsApp, não existia Telegram, o único aplicativo de mensagem que todo mundo usava era SMS. De qualquer forma, quando eles começaram a experimentar, a recomendar usuários novos pra gente seguir por volta de 2010, foi com um sistema que chamaram carinhosamente de WTF. What the fuck? Mentira. Era rootfollow. A arquitetura era como nessa figura. Segundo o paper, no seu núcleo tinha esse Castlevary, que é um motor de processamento de grafos em memory, ou seja, que mantém os grafos em memória. Isso é importante porque a maioria dos algoritmos conhecidos pra lidar com grafos, assume ter o grafo todo em memória. Guarde essa informação. Esse projeto, Castlevary, é open source e podemos encontrar no GitHub deles. Última atualização foi de 2021 e não é mais utilizado hoje. Continuando, esse Castlevary operava em snapshots de grafos de seguidores carregados do HDFS, que é o sistema de arquivos distribuídos do Hadoop. Quem já lidou com Big Data, Data Lakes, certamente já esbarrou em instalações de HDFS e no Twitter não era diferente. Só entenda que é um sistema de arquivos distribuído, como se você tivesse uma parte de redação única e espalhada em múltiplos servidores. É um nível acima de um simples RAID. Os grafos de seguidores vinham de outro banco de dados, chamado FlockDB, que fazia a ingestão desses dados no HDFS. FlockDB, acho que foi o primeiro banco de dados de grafos que o Twitter criou e soltou como open source. O que aconteceu foi, eles começaram com qualquer tech startup normal sem saber o que o produto ia se tornar. No começo, usavam um banco de dados normal, mais psico ao mesmo. Mas quando começaram a fazer sucesso, e ver o problema, era um problema de uso. era maior que um banco relacional, começaram a pesquisar alternativas e o primeiro resultado foi um banco de dados de grafos, o FlockDB. O FlockDB dava mais performance para operações de inserção, updates, já que não precisava das garantias ac de um banco relacional. Tinha operações extras para lidar com grafos, como navegar por nós e pelos vértices. E vocês têm que entender que carregar um grafo em memória pode ser pesado. Imagine sua própria conta, você tem dezenas de tweets, você segue dezenas de pessoas, pessoas tem dezenas de tweets. Cada um desses tweets tem contagens de likes, retweets, alguns tweets são replies para outros tweets e para calcular qual deles é mais relevante ou menos relevante, o ideal é ter toda essa estrutura em memória para conseguir navegar por toda essa informação. Agora pensa num banco de dados relacional como o mais ciclo. Como você resolveria esse problema? Qualquer solução que você imaginar em SQL pode ter certeza, já foi testado e sabemos que não vai escalar. Esse é um dos poucos cenários que podemos afirmar que um banco de dados relacional não é adequado. Segundo a página de GitHub do FlockDB, até 2010 o cluster deles armazenava mais de 13 bilhões de bordas e sustentava picos de tráfico de mais de 20 mil escritas por segundo e mais de 100 mil operações de leitura por segundo. Mas rapidamente chegou um ponto onde isso já não era mais suficiente, foi quando começaram a puxar esses dados para a HDFS e processando nesse novo projeto que chamaram de Castle Vary, um segundo projeto em Java para lidar com Big Graphs, grafos com bilhões de nozes e vértices. Foi a época que as celebridades gigantes começaram a viver no Twitter. Pense nível Kardashians. Voltando ao paper, a camada de armazenamento do Castle Vary dava acesso aos grafos via queries, pesquisas baseadas em vértices. Um SQL especializado em grafos. E em cima disso, um motor de recomendação computava sugestões de rootfollow, o WTF, ou quem seguir. Foi assim que nos primórdios do Twitter ganhamos sugestões de novas contas para poder seguir. Um dos objetivos desse projeto era ver se era possível manter os grafos em memória, em memória. A pergunta que eles não tinham resposta é, será que o crescimento do volume de dados desses grafos seria mais lento que a lei de Moore? Se for o preço de memória iria baratiar mais rápido do que os dados cresciam e daria para computar tudo em memória, que é mais simples do que ter que lidar com discos, particionamento, paging, cache e tudo mais. Como o paper diz, considere um grafo com 10 bilhões de bordas. Mesmo uma representação ingênua simples ocuparia algo na faixa de 80 GB que um único servidor consegue aguentar. Você já trabalhou com o servidor? Esse ano que tem 80 GB, se sim, diga nos comentários que empresa é pra todo mundo entender meu ponto. Uma das limitações, mesmo naquela época, é que essas sugestões de recomendação de quem seguir eram calculados em batch, uma vez por dia. Não era em tempo real como é hoje, ou seja, você só tinha recomendações novas uma vez por dia. De qualquer forma, via muita experimentação, muitos testes AB, eles chegaram em dois conceitos que usam até hoje que vale explicar. O primeiro se chama Círculo de Confiança e o segundo foi a adoção de Salsa, que é Stochastic Approach for Link Structure Analysis ou Analise de Estrutura de Links Usando Processos Estocásticos.
