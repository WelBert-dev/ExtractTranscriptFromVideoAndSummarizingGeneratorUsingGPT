 Pra ter recomendações ainda melhores eles queriam usar mais sinais dos logs de comportamento dos usuários comportamentos de likes retweets replies follow e tudo mais. Mas isso torna os metadados do grafo ainda maiores e a premissa da primeira versão que era tentar manter tudo em memória já não cabia mais. Daí veio a decisão de tentar usar Hadoop em particular a Pache Pig que pra quem não sabe é tipo um compilador que produz sequências de programas map reduce. Hadoop é uma plataforma de tecnologias criadas pra lidar com o problema de Big Data no auge da bolha da internet no começo dos anos 2000. Pra muita gente ainda hoje o conceito de Big Data não é muito palpável porque ninguém lida com isso no dia a dia. É de produto pra produto mas quando estamos gerando coisa de gigabytes por dia de dados terabytes por mês estamos em território de Big Data. O que você faz com banco de dados que tem centenas de terabytes. Data e chora porque não importa qual o bom seja seus índices e queries sua aplicação vai ficar absurdamente lenta só pelo tamanho. Nessa ordem de grandeza estamos falando que não é possível manter todos os dados numa única máquina mesmo sendo um servidor grande seja por limitação física ou de custo. Aí a única solução é a estratégia de Napoleão particionar e dividir esse tanto de dados em múltiplos servidores e daí conquistar. Por isso a base do Hadoop começa com o HDFS que é o Hadoop File System um sistema de arquivos distribuído. Agora o problema é o seguinte na faculdade você aprendeu a lidar com dados todos no mesmo lugar por exemplo digamos que você tem uma lista de produtos e preços e queremos achar o produto mais barato como que faz. Eles escolham um algoritmo qualquer de ordenação como um quick sort ou merge sorte da vida e pega o primeiro elemento que vai ser mais barato. Pra fazer isso precisamos iterar elemento a elemento da lista pelo menos uma vez. Mas e agora e citamos com cinco servidores e essa lista foi dividida por cinco. Intuitivamente poderíamos pensar beleza eu vou ter que repetir a ordenação uma vez em cada máquina. Daí no final eu vou ter uma lista parcial com os cinco produtos mais baratos achados em cada um dos cinco servidores daí eu faço uma última ordenação nessa lista e finalmente eu acho o mais barato. Cada um dos servidores vai rodar em um quinto do tempo do que se fosse a lista completa mas no final ainda tem pelo menos mais uma operação de ordenação extra e aí eu tenho resultado. Então seja tecnicamente um pouco mais lento rodar em vários servidores separados mas vai ser menos caro. Isso de rodar um algoritmo em pedaços dos dados mapear os resultados parciais e depois reduzir na resposta final é o que a grosso modo chamamos de map reduce que é parte fundamental de como fazer processamento de dados em radupe. Novamente eu não vou detalhar map reduce então anote para pesquisar mais depois. O map reduce ficou famoso porque depois do pade rank é como o Google conseguiu operar em larga escala e deixou todo mundo no chinelo no começo dos anos 2000. Hoje em dia é como toda grande empresa de tecnologia funciona mais 20 anos atrás essa forma de pensar em processamento de dados foi fundamental para possibilitar termos produtos que aceitam quantidades massivas de pessoas e transações literalmente ultrapassando a população total de diversos países. Tem mais gente no Twitter do que a população dos Estados Unidos metade da população do mundo está em produtos da meta. Por isso para a segunda versão do motor de recomendação o Twitter deixou o flockdb e wtf para trás e perseguir o tal projeto que é sovery para rodar em cima do hdfs do radupe usando map reduce mas eles tiveram vários problemas. Mesmo em 2012 já era sabido que grafos não performavam bem em radupe. Isso porque a maioria dos algoritmos conhecidos para processar grafos são iterativos. Lembra que eu falei para guardar essa informação?
