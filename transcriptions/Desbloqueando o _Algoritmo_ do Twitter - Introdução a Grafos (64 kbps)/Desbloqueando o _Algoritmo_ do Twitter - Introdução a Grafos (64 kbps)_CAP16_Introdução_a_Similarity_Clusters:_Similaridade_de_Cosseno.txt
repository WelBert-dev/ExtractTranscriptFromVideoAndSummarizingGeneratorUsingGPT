 Agora que temos uma noção melhor do contexto histórico dos projetos e papers de pesquisas gerados, podemos começar a entender melhor o código liberado. Esse código é um conjunto de diversos subprojetos. Se abrirmos o primeiro ritmo na raiz do repositorio, eles descreveram o que são cada um desses projetos. Por exemplo, SimClusterN é o serviço que retorna candidatos de recomendações de tweets dado em badgings de um cluster de similaridade. Esse serviço usa o algoritmo de similaridade aproximado de cossendo. Já vou explicar isso. Segundo o ritmo, um job constrói uma apéamento entre SimClusters e Tweets. O job grava os top 400 Tweets de um SimCluster e os tops sem SimClusters pra um tweet. Pontuação de favoritos e pontuação de seguidores são dois tipos de pontuação que um tweet pode ter. E os tops sem SimCluster baseados nessas pontuações é que chamam de tweet SimCluster embedding. A similaridade de cossendo entre dois embeddings apresenta o nível de relevância de dois tweets no espaço de SimClusters. A pontuação varia de 0 a 1. A alta pontuação de cossendo de maior que 0.7 significa que os usuários que gostam de dois desses tweets compartilham os mesmos SimClusters. Vamos lá. Similaridade de cossendo é um conceito de ause bralinear pra medir quão similar são dois vetores em termos de direção e magnitude. Eu gosto desse conceito porque eu expliquei isso desde pelo menos 2015 até minha última palestra pública em 2019. Quem comeu a minha palestra restrição igual a inovação vai se lembrar disso e eu vou aproveitar pra usar os slides que eu usava nessa palestra. Comece visualizando a página inicial do Google o campo onde se digita palavras chaves. Podemos digitar Apple e recebemos páginas indexadas da web que tem essa palavra. Um amador que acabou de aprender SQL poderia pensar que é alguma coisa parecida com Select, URL, From Page, Square Text, Like, % Apple por cento. Essa é a solução que faríamos no dos anos 90 antes de Larry Page, Sergey Brin inventar em Page Rank, antes de John Kleinberg inventar hits e essa linha de SQL de fato vai achar todas as partes onde aparece a palavra Apple, mas não vai saber como ordenar. Qual delas é mais relevante e deveria estar no topo da lista. Pra saber relevância precisamos de álgebra linear. Tudo depende de como você enxerga o que é uma página web. Todo mundo enxerga um documento, uma coleção de palavras que é o texto desse documento. Digamos que nosso banco de dados só tem três documentos indexados como nesse slide. Pra simplificar bastante, vamos assumir que nosso vocabulário é bem limitado e os documentos só tem as palavras Apple e banana. O documento 1 tem quatro apples e uma banana, o documento 2 tem três bananas e um Apple e o documento 3 tem cinco bananas e nenhum Apple. Podemos representar esses documentos como vetores como nesse slide. Próximo passo, criamos um índice que é uma lista que diz em quais documentos aparece a palavra Apple e quais aparece banana. Pense em algo como índice do seu banco de dados MySQL ou Postgres ou SQL Server da vida. Agora vamos digitar a palavra Apple no campo de pesquisa desse nosso Google de mentira. O truque começa a representar essa pesquisa, essa quele também como um vetor, e no caso Apple sendo 1 e banana sendo 0. É como se fosse um documento também. Pelo índice podemos descartar o documento 3 já que Apple não aparece nenhuma vez, mas ainda temos o documento 1 e o documento 2. Qual deles é o mais relevante para aparecer no topo da lista? E de bater o olho sabemos que é o documento 1 porque aparece a palavra Apple quatro vezes e no documento 2 aparece uma vez. Mas como o algoritmo conseguiria computar isso? Pra isso desenhamos esses vetores num espaço vetorial, um vector space. E no caso só temos duas dimensões porque só temos duas palavras no nosso vocabulário. Apple, eixo x, banana e o eixo y. Então o vetor que representa o documento 1 seria assim, quatro unidades pra direita no eixo x e uma unidade pra cima no eixo y. Já o documento 2 seria só uma unidade pra direita no eixo x, mas três unidades pra cima no eixo y. Finalmente o vetor da query é só uma unidade pra direita no eixo x, só uma palavra Apple, entenderam? O lance com vetores é que agora temos ângulos entre o vetor de pesquisa e os vetores de documentos. E pra calcular a similaridade de cc, não usamos dot product que é produto escalar. Funciona assim, vamos multiplicar Apple com Apple, um banana e somar os dois. Então um vetor de pesquisa é um em zero, um Apple e zero bananas. E vetor do documento 1 é quatro Apple e uma banana. Então a conta seria uma vezes quatro, mais zero vezes um que dá quatro. Esse é o produto escalar. Daí fazemos a mesma coisa entre o vetor de pesquisa de novo e o vetor do documento 2. Então o produto escalar vai ser um Apple vezes um Apple, mais zero banana dando um. Como o produto escalar quatro, do documento 1 é maior que o 1 do documento 2, sabemos que o documento 1 tem uma similaridade com a pesquisa e portanto é o mais relevante pra aparecer no topo da lista. Esse exemplo é super trivial porque só usamos duas palavras, então temos só duas dimensões. Mas vamos complicar, e se tivéssemos um novo documento indexado que tem a palavra coco, o vetor dele poderia ser um Apple, zero banana e dois cocos. Todos os outros vetores precisam ser atualizados pra contê coco, mas no caso de zero cocos, então não muda nada. Pra representar o espaço vetor, ele é o precisamos de uma nova dimensão Z. O vetor desse novo documento seria desenhado com uma unidade pra direita no eixo X e duas unidades pra baixo no eixo Z. E podemos calcular o produto escalar entre a pesquisa e esse novo documento também. Em geometria a gente consegue desenhar no eixo Z três dimensões. Mas claro, o vocabulário de uma língua como o português é muito mais que só três palavras. No dicionário como o aurelio, vai ter mais de 500 mil palavras, portanto podemos ter potencialmente 500 mil dimensões nesse espaço vetorial. Na realidade vai ser a quantidade de palavras únicas que tem no documento ou parte na web de verdade. O script desse documento, por exemplo, tem mais de 3 mil palavras únicas, é um vetor de 3 mil dimensões. Felizmente, o escalar é uma operação muito rápida de se calcular em qualquer computador moderno, só somas e multiplicações simples. E é assim que funciona, a base de todo tipo de algoritmo de relevância e recomendação. É isso que tem por baixo da biblioteca Lucini que eu falei antes. E se olhar a documentação de um elástico, vai ver que menciona a VSM ou Vector Space Model, que é esse gráfico que eu vim mostrando muito maior do que os três do exemplo. E é isso que esse projeto Sim Cluster vai fazer, só que com vetores de usuários e tweets. No caso do Twitter, esse primeiro projeto se chama Sim Cluster N. Sim Cluster é Similarity Cluster e esse N provavelmente significa Approximately nearest neighbor, ou o vizinho mais próximo aproximado. Em Machine Learning e Data Mining, pesquisa de vizinho mais próximo é o problema de encontrar o ponto de dado mais próximo em um espaço de muitas dimensões dado um certo ponto de pesquisa onde soluções exatas são enviáveis pelo tamanho do conjunto de dados. Algoritmos de vizinho próximo aproximado como Locality Sense TV Hatching ou LSH e métodos baseados em árvore como KDTrees oferecem uma forma eficiente de resolver esse problema fazendo um trade-off de algum grau de precisão por uma melhoria significativa em performance. Então é mais uma otimização do Twitter para ganhar performance, mais ou menos como o Random Walk do algoritmo de salsa que eu expliquei antes. Nesse subprojeto o conceito importante são Embedding Spaces que tem com o objetivo responder a pergunta quais tweets e usuários são similares aos meus interesses. Embedding funciona gerando representações numéricas dos interesses dos usuários e conteúdos de tweet. Daí podemos calcular a similaridade entre quaisquer dois usuários, tweets ou pares de usuários e tweets e espaço de embedding. Justamente um dos espaços de embeddings mais úteis do Twitter são SimClusters. SimClusters descobrem comunidades ancoradas por um grupo de usuários e influentes usando um algoritmo customizado de factorização de matrizes. Existem uma 145 mil comunidades que são atualizadas a cada três semanas. Usuários e tweets são representados em espaço de comunidades e pertencem a múltiplas comunidades. Elas podem ter algumas milhares até milhões de usuários. Segundo o blog de engenharia essa imagem representa algumas das maiores comunidades.
